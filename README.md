# ğŸš€ FastTuneWhisper

**FastTuneWhisper** is a production-ready multimodal search system that enables fast **text** and **image-based** search over YouTube videos. It leverages **fine-tuned Whisper**, **CLIP embeddings**, and **ChromaDB** to build a searchable video content index.

<br>

## ğŸ” Key Features

* ğŸ¥ Accepts YouTube video URLs
* ğŸ§  Scene-based frame extraction
* ğŸ“ Audio transcription using **Fast/Fine-tuned Whisper**
* ğŸ’¡ Text & image embeddings using **CLIP (ViT-B/32)**
* ğŸ” Multimodal search support: **text-to-video** and **image-to-video**
* âš¡ FastAPI backend + Streamlit frontend
* ğŸ—ƒ Persistent vector storage with **ChromaDB**

<br>

## ğŸ“ Project Structure

```
FastTuneWhisper/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ audio_processing.py         
â”‚   â”œâ”€â”€ chromadb_functions.py       
â”‚   â”œâ”€â”€ download_video.py           
â”‚   â”œâ”€â”€ extract_scenes_from_video.py
â”‚   â”œâ”€â”€ extract_text.py             
â”‚   â”œâ”€â”€ image_text_emb.py           
â”‚   â”œâ”€â”€ search_functions.py         
â”‚   â”œâ”€â”€ video_fusion_search.py      
â”‚   â””â”€â”€ main.py                     
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ frontend.py                 
â”‚
â”œâ”€â”€ notebook/
â”‚   â”œâ”€â”€ text_processing.ipynb       
â”‚   â”œâ”€â”€ image_processing.ipynb      
â”‚   â””â”€â”€ image_processing2.ipynb     
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

<br>

## âš™ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/jigneshshiyal/FastTuneWhisper.git
cd FastTuneWhisper
```

### 2. Create a virtual environment

```bash
python -m venv env
source env/bin/activate  # Windows: .\env\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Start FastAPI backend

```bash
cd backend
uvicorn main:app --reload --port 8000
```

### 5. Start Streamlit frontend

```bash
cd ../frontend
streamlit run frontend.py
```

<br>

## ğŸ§ª How It Works

1. ğŸ¬ **Embed Video** â€“ Paste a YouTube URL, system extracts audio + scenes, then generates embeddings.
2. ğŸ” **Search** â€“ Input a text prompt or upload an image to find matching timestamps.
3. ğŸ•’ **Jump** â€“ Click on the returned timestamp to jump directly into the relevant video moment.

<br>

## ğŸ§  Models & Libraries

* ğŸ—£ **Whisper (distil-large-v3)** â€“ For fast and accurate transcription
* ğŸ–¼ **OpenAI CLIP (ViT-B/32)** â€“ Unified text-image embeddings
* ğŸ“¦ **ChromaDB** â€“ Vector database for fast similarity search
* âš™ï¸ **SceneDetect + OpenCV** â€“ Scene-based keyframe extraction

<br>

## âœ… Requirements

* Python 3.8+
* `ffmpeg` (ensure it's in your PATH)
* Internet access for downloading YouTube videos and model weights

<br>

## ğŸ“Œ Notebooks

Located in the `notebook/` folder:

* Analyze and visualize text/image embeddings
* Useful for testing and debugging the CLIP embedding space

<br>

## ğŸš€ Future Ideas

* Export ONNX for in-browser or mobile inference
* Add video thumbnails to search results
* Support for multilingual transcription (Whisper large-v3)

<br>

## ğŸ“„ License

MIT License Â© 2025 Jignesh Shiyal

