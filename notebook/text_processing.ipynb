{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47bf7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def extract_audio(video_path, output_audio_path=\"output_audio.wav\"):\n",
    "    \"\"\"\n",
    "    Extract audio from a video file and convert it to 16kHz mono WAV.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path (str): Path to the input video file (.webm or .mp4)\n",
    "    - output_audio_path (str): Path to save the extracted WAV audio\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "    \n",
    "    # ffmpeg command\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\", video_path,\n",
    "        \"-vn\",  # disable video\n",
    "        \"-acodec\", \"pcm_s16le\",  # uncompressed audio format\n",
    "        \"-ar\", \"16000\",  # sample rate 16kHz\n",
    "        \"-ac\", \"1\",  # mono channel\n",
    "        output_audio_path,\n",
    "        \"-y\"  # overwrite output if exists\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"✅ Audio extracted and saved to: {output_audio_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"❌ ffmpeg failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54819d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio extracted and saved to: output_audio.wav\n"
     ]
    }
   ],
   "source": [
    "video_file_path = \"../notebook/input.webm\"\n",
    "extract_audio(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0731b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jignesh.shiyal\\Documents\\video_search_v3\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import difflib\n",
    "\n",
    "# Load the model (on CPU with float32 precision)\n",
    "model = WhisperModel(\"distil-large-v3\", compute_type=\"int8\", device=\"cpu\")  # or \"float32\" for better accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ace0b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s -> 3.00s]  The neat thing about working in machine learning is that every few years,\n",
      "[3.00s -> 7.00s]  somebody invent something crazy that makes you totally reconsider what's possible,\n",
      "[7.00s -> 12.00s]  like models that can play Go or generate hyper-realistic faces.\n",
      "[12.00s -> 15.00s]  And today, the mind-blowing discovery that's rocking everyone's world\n",
      "[15.00s -> 17.00s]  is a type of neural network called a transformer.\n",
      "[17.00s -> 20.00s]  Transformers are models that can translate text,\n",
      "[20.00s -> 23.00s]  write poems and op-eds, and even generate computer code.\n",
      "[23.00s -> 26.00s]  These have been used in biology to solve the protein folding problem.\n",
      "[26.00s -> 29.00s]  Transformers are like this magical machine-learning hammer\n",
      "[29.00s -> 31.00s]  that seems to make every problem into a nail.\n",
      "[31.00s -> 36.00s]  If you've heard of the trendy new ML models, BERT, or GPT3, or T5,\n",
      "[36.00s -> 38.00s]  all of these models are based on transformers.\n",
      "[38.00s -> 41.00s]  So if you want to stay hip in machine learning,\n",
      "[41.00s -> 43.00s]  and especially in natural language processing,\n",
      "[43.00s -> 45.00s]  you have to know about the transformer.\n",
      "[45.00s -> 48.00s]  So in this video, I'm going to tell you about what transformers are,\n",
      "[48.00s -> 50.00s]  how they work, and why they've been so impactful.\n",
      "[50.00s -> 52.00s]  Let's get to it.\n",
      "[52.00s -> 53.00s]  So what is a transformer?\n",
      "[53.00s -> 55.00s]  It's a type of neural network architecture.\n",
      "[55.00s -> 58.00s]  To recap, neural networks are a very effective type of model\n",
      "[58.00s -> 63.00s]  of model for analyzing complicated data types like images, videos, audio, and text.\n",
      "[63.00s -> 66.00s]  But there are different types of neural networks optimized for different types of data.\n",
      "[66.00s -> 70.00s]  Like if you're analyzing images, you typically use a convolution neural network,\n",
      "[70.00s -> 75.00s]  which is designed to vaguely mimic the way that the human brain processes vision.\n",
      "[75.00s -> 79.00s]  And since around 2012, neural networks have been really good at solving vision tasks,\n",
      "[79.00s -> 81.00s]  like identifying objects and photos.\n",
      "[81.00s -> 85.00s]  But for a long time, we didn't have anything comparably good for analyzing language,\n",
      "[85.00s -> 87.00s]  whether for translation or text summarization or text generalization.\n",
      "[87.00s -> 89.00s]  or text generation.\n",
      "[89.00s -> 92.00s]  And this is a problem because language is the primary way that humans communicate.\n",
      "[92.00s -> 96.00s]  You see, until Transformers came around, the way we use deep learning to understand text\n",
      "[96.00s -> 102.00s]  was with a type of model called a recurrent neural network, or an RNN, that looks something like this.\n",
      "[102.00s -> 106.00s]  Let's say you wanted to translate a sentence from English to French.\n",
      "[106.00s -> 110.00s]  An RNN would take as input an English sentence and process the words one at a time,\n",
      "[110.00s -> 113.00s]  and then sequentially spit out their French counterparts.\n",
      "[113.00s -> 116.00s]  The key word here is sequential.\n",
      "[116.00s -> 117.00s]  In language, the order of language,\n",
      "[117.00s -> 120.00s]  words matters and you can't just shuffle them around.\n",
      "[120.00s -> 123.00s]  For example, the sentence, Jane went looking for trouble.\n",
      "[123.00s -> 125.00s]  It means something very different than the sentence,\n",
      "[125.00s -> 127.00s]  trouble went looking for Jane.\n",
      "[127.00s -> 130.00s]  So any model that's going to deal with language has to capture word order,\n",
      "[130.00s -> 134.00s]  and recurrent neural networks do this by looking at one word at a time sequentially.\n",
      "[134.00s -> 136.00s]  But RNNs had a lot of problems.\n",
      "[136.00s -> 140.00s]  First, they never really did well at handling large sequences of text,\n",
      "[140.00s -> 142.00s]  like long paragraphs or essays.\n",
      "[142.00s -> 146.00s]  By the time they were analyzing the end of a paragraph, they'd forget what happened in the beginning.\n",
      "[146.00s -> 147.00s]  And even worse,\n",
      "[147.00s -> 149.00s]  R&Ns were pretty hard to train.\n",
      "[149.00s -> 152.00s]  Because they processed words sequentially, they couldn't paralyze well,\n",
      "[152.00s -> 156.00s]  which means that you couldn't just speed them up by throwing lots of GPUs at them.\n",
      "[156.00s -> 160.00s]  And when you have a model that's slow to train, you can't train it on all that much data.\n",
      "[160.00s -> 162.00s]  This is where the transformer changed everything.\n",
      "[162.00s -> 166.00s]  They were a model developed in 2017 by researchers at Google and the University of Toronto,\n",
      "[166.00s -> 169.00s]  and they were initially designed to do translation.\n",
      "[169.00s -> 173.00s]  But unlike recurrent neural networks, you could really efficiently paralyze transformers.\n",
      "[173.00s -> 176.00s]  And that meant that with the right hardware, you could train some really big models.\n",
      "[176.00s -> 177.00s]  models.\n",
      "[177.00s -> 178.00s]  How big?\n",
      "[178.00s -> 179.00s]  Really big.\n",
      "[179.00s -> 183.00s]  Remember GPT3, that model that writes poetry and code and has conversations?\n",
      "[183.00s -> 189.00s]  That was trained on almost 45 terabytes of text data, including like, almost the entire public\n",
      "[189.00s -> 190.00s]  web.\n",
      "[190.00s -> 193.00s]  So if you remember anything about Transformers, let it be this.\n",
      "[193.00s -> 198.00s]  Combine a model that scales really well with a huge data set, and the results will probably blow your\n",
      "[198.00s -> 199.00s]  mind.\n",
      "[199.00s -> 201.00s]  So how do these things actually work?\n",
      "[201.00s -> 204.00s]  From the diagram in the paper, it should be pretty clear.\n",
      "[204.00s -> 205.00s]  Or maybe not.\n",
      "[205.00s -> 206.00s]  Actually.\n",
      "[206.00s -> 208.00s]  it's simply than you might think.\n",
      "[208.00s -> 211.00s]  There are three main innovations that make this model work so well.\n",
      "[211.00s -> 216.00s]  Positional encodings and attention, and specifically a type of attention called self-attention.\n",
      "[216.00s -> 219.00s]  Let's start by talking about the first one, positional encodings.\n",
      "[219.00s -> 222.00s]  Let's say we're trying to translate text from English to French.\n",
      "[222.00s -> 226.00s]  Positional encoding is the idea that instead of looking at words sequentially,\n",
      "[226.00s -> 229.00s]  you take each word in your sentence and before you feed it into the neural network,\n",
      "[229.00s -> 231.00s]  you slap a number on it.\n",
      "[231.00s -> 234.00s]  One, two, three, depending on what number the word is in the sentence.\n",
      "[234.00s -> 236.00s]  In other words, you store information about a word.\n",
      "[236.00s -> 240.00s]  word order in the data itself rather than in the structure of the network.\n",
      "[240.00s -> 244.40s]  Then as you train the network on lots of text data, it learns how to interpret those positional\n",
      "[244.40s -> 250.88s]  encodings. In this way, the neural network learns the importance of word order from the data.\n",
      "[250.88s -> 254.64s]  This is a high-level way to understand positional encodings, but it's an innovation that really\n",
      "[254.64s -> 260.48s]  helped make transformers easier to train than RNMs. The next innovation in this paper is a concept\n",
      "[260.48s -> 265.28s]  called attention, which you'll see used everywhere in machine learning these days. In fact, the title of the\n",
      "[265.28s -> 270.32s]  the original transformer paper is attention is all you need. So, the agreement on\n",
      "[270.32s -> 275.84s]  the European Economic Area was signed in August 1992. Did you know that? That's the\n",
      "[275.84s -> 280.24s]  example sentence given in the original paper, and remember, the original transformer was designed\n",
      "[280.24s -> 285.28s]  for translation. Now imagine trying to translate that sentence to French. One bad way to\n",
      "[285.28s -> 290.24s]  translate text is to try to translate each word one for one. But in French, some words are flipped,\n",
      "[290.24s -> 295.20s]  like in the French translation, European comes before economic. Plus, French is a language.\n",
      "[295.28s -> 300.32s]  that has gendered agreement between words. So the word European needs to be in the feminine\n",
      "[300.32s -> 305.52s]  form to match with la zone. The attention mechanism is a neural network structure that allows a\n",
      "[305.52s -> 309.92s]  text model to look at every single word in the original sentence when making a decision about how\n",
      "[309.92s -> 314.72s]  to translate a word in the output sentence. In fact, here's a nice visualization from that paper\n",
      "[314.72s -> 319.52s]  that shows what words in the input sentence the model is attending to when it makes predictions\n",
      "[319.52s -> 325.26s]  about a word for the output sentence. So when the model outputs the word European, it's\n",
      "[325.28s -> 330.48s]  looking at the input words European and economic. You can think of this diagram as a sort of\n",
      "[330.48s -> 335.92s]  heat map for attention. And how does the model know which words it should be attending to? It's\n",
      "[335.92s -> 340.56s]  something that's learned over time from data. By seeing thousands of examples of French and English\n",
      "[340.56s -> 344.88s]  sentence pairs, the model learns about gender and word order and plurality and all of that\n",
      "[344.88s -> 349.92s]  grammatical stuff. So we talked about two key transformer innovations, positional encoding and\n",
      "[349.92s -> 355.12s]  attention. But actually, attention had been invented before this paper. The real innovation\n",
      "[355.12s -> 359.52s]  in Transformers with something called self-attention, a twist on traditional attention.\n",
      "[360.32s -> 364.32s]  The type of attention we just talked about had to do with aligning words in English and French,\n",
      "[364.32s -> 368.40s]  which is really important for translation. But what if you're just trying to understand the\n",
      "[368.40s -> 373.28s]  underlying meaning in language so that you can build a network that can do any number of language\n",
      "[373.28s -> 378.48s]  tasks? What's incredible about neural networks like Transformers is that as they analyze\n",
      "[378.48s -> 384.08s]  tons of text data, they begin to build up this internal representation or understanding of language\n",
      "[384.08s -> 389.52s]  automatically. They might learn, for example, that the words programmer and software engineer\n",
      "[389.52s -> 394.32s]  and software developer are all synonymous, and they might also naturally learn the rules of grammar\n",
      "[394.32s -> 399.04s]  and gender and tense and so on. The better this internal representation of language the neural\n",
      "[399.04s -> 404.16s]  network learns, the better it will be at any language task. And it turns out that attention can\n",
      "[404.16s -> 408.80s]  be a very effective way to get a neural network to understand language if it's turned on the input\n",
      "[408.80s -> 413.68s]  text itself. Let me give you an example. Take these two sentences.\n",
      "[414.08s -> 418.32s]  Can I have the check versus looks like I just crashed the server?\n",
      "[418.32s -> 422.56s]  The word server here means two very different things, and I know that because I'm looking at the\n",
      "[422.56s -> 427.92s]  context of the surrounding words. Self-attention allows a neural network to understand a word\n",
      "[427.92s -> 432.40s]  in the context of the words around it. So when a model processes the word server in the first\n",
      "[432.40s -> 438.00s]  sentence, it might be attending to the word check, which helps it disambiguate from a human server\n",
      "[438.00s -> 442.40s]  versus a metal one. In the second sentence, the model might be attending to the word crashed\n",
      "[442.40s -> 446.32s]  to determine that this server is a machine. Self-attention can also help neural networks\n",
      "[446.32s -> 452.16s]  disambiguate words, recognize parts of speech, and even identify word tense. This, in a nutshell,\n",
      "[452.16s -> 457.68s]  is the value of self-attention. So to summarize, transformers boil down to positional\n",
      "[457.68s -> 464.00s]  encodings, attention, and self-attention. Of course, this is a 10,000-foot look at transformers,\n",
      "[464.00s -> 468.56s]  but how are they actually useful? One of the most popular transformer-based models is called\n",
      "[468.56s -> 472.24s]  Bert, which was invented just around the time that I joined Google in 2018.\n",
      "[472.40s -> 477.28s]  Burt was trained on a massive text corpus and has become this sort of general pocket\n",
      "[477.28s -> 482.64s]  knife for NLP that can be adopted to a bunch of different tasks, like text summarization,\n",
      "[482.64s -> 486.94s]  question answering, classification, and finding similar sentences.\n",
      "[486.94s -> 491.34s]  It's used in Google Search to help understand search queries, and it powers a lot of Google Cloud's\n",
      "[491.34s -> 495.12s]  NLP tools like Google Cloud, AutoMell natural language.\n",
      "[495.12s -> 499.80s]  Burt also proved that you could build very good models on unlabel data, like text scraped from\n",
      "[499.80s -> 501.60s]  Wikipedia or Reddit.\n",
      "[501.60s -> 507.36s]  This is called semi-supervised learning, and it's a big trend in machine learning right now.\n",
      "[507.36s -> 511.96s]  So if I've sold you on how cool transformers are, you might want to start using them in your app.\n",
      "[511.96s -> 513.04s]  No problem.\n",
      "[513.04s -> 517.04s]  Transferflow Hub is a great place to grab pre-trained transformer models like Bert.\n",
      "[517.04s -> 521.70s]  You can download them for free in multiple language and drop them straight into your app.\n",
      "[521.70s -> 525.64s]  You can also check out the popular Transformers Python Library, built by the company\n",
      "[525.64s -> 526.64s]  Hugging Face.\n",
      "[526.64s -> 530.04s]  That's one of the community's favorite ways to train and use transformer models.\n",
      "[530.04s -> 531.58s]  For more transformer tips, check out my\n",
      "[531.60s -> 534.48s]  my blog post linked below and thanks for watching.\n"
     ]
    }
   ],
   "source": [
    "# Transcribe audio with timestamp support\n",
    "segments, info = model.transcribe(\"../notebook/output_audio.wav\", beam_size=5, language=\"en\")\n",
    "\n",
    "# Store transcript and segment data\n",
    "transcript_segments = []\n",
    "\n",
    "for segment in segments:\n",
    "    entry = {\n",
    "        \"start\": segment.start,\n",
    "        \"end\": segment.end,\n",
    "        \"text\": segment.text.strip()\n",
    "    }\n",
    "    transcript_segments.append(entry)\n",
    "    print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39dbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip  # pip install git+https://github.com/openai/CLIP.git\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "# Prepare text embeddings (same as transcript_segments)\n",
    "texts = [seg[\"text\"] for seg in transcript_segments]\n",
    "text_tokens = clip.tokenize(texts).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.encode_text(text_tokens)\n",
    "    text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)  # Normalize\n",
    "\n",
    "\n",
    "def search_transcript_clip(query, top_k=3):\n",
    "    with torch.no_grad():\n",
    "        query_token = clip.tokenize([query]).to(device)\n",
    "        query_embedding = model.encode_text(query_token)\n",
    "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        cosine_scores = (query_embedding @ text_embeddings.T).squeeze(0)\n",
    "\n",
    "        top_results = torch.topk(cosine_scores, k=top_k)\n",
    "\n",
    "        results = []\n",
    "        for score, idx in zip(top_results.values, top_results.indices):\n",
    "            match = transcript_segments[idx]\n",
    "            results.append({\n",
    "                \"score\": float(score),\n",
    "                \"start\": match[\"start\"],\n",
    "                \"end\": match[\"end\"],\n",
    "                \"text\": match[\"text\"]\n",
    "            })\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "126cfb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ [265.28s → 270.32s] (Score: 0.87) → the original transformer paper is attention is all you need. So, the agreement on\n",
      "⏱ [43.00s → 45.00s] (Score: 0.86) → you have to know about the transformer.\n",
      "⏱ [52.00s → 53.00s] (Score: 0.85) → So what is a transformer?\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Try it out\n",
    "query = \"transformer paper\"\n",
    "matches = search_transcript_clip(query)\n",
    "\n",
    "for match in matches:\n",
    "    print(f\"⏱ [{match['start']:.2f}s → {match['end']:.2f}s] (Score: {match['score']:.2f}) → {match['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d0df58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d138f26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0.0,\n",
       "  'end': 3.0,\n",
       "  'text': 'The neat thing about working in machine learning is that every few years,'},\n",
       " {'start': 3.0,\n",
       "  'end': 7.0,\n",
       "  'text': \"somebody invent something crazy that makes you totally reconsider what's possible,\"},\n",
       " {'start': 7.0,\n",
       "  'end': 12.0,\n",
       "  'text': 'like models that can play Go or generate hyper-realistic faces.'},\n",
       " {'start': 12.0,\n",
       "  'end': 15.0,\n",
       "  'text': \"And today, the mind-blowing discovery that's rocking everyone's world\"},\n",
       " {'start': 15.0,\n",
       "  'end': 17.0,\n",
       "  'text': 'is a type of neural network called a transformer.'},\n",
       " {'start': 17.0,\n",
       "  'end': 20.0,\n",
       "  'text': 'Transformers are models that can translate text,'},\n",
       " {'start': 20.0,\n",
       "  'end': 23.0,\n",
       "  'text': 'write poems and op-eds, and even generate computer code.'},\n",
       " {'start': 23.0,\n",
       "  'end': 26.0,\n",
       "  'text': 'These have been used in biology to solve the protein folding problem.'},\n",
       " {'start': 26.0,\n",
       "  'end': 29.0,\n",
       "  'text': 'Transformers are like this magical machine-learning hammer'},\n",
       " {'start': 29.0,\n",
       "  'end': 31.0,\n",
       "  'text': 'that seems to make every problem into a nail.'},\n",
       " {'start': 31.0,\n",
       "  'end': 36.0,\n",
       "  'text': \"If you've heard of the trendy new ML models, BERT, or GPT3, or T5,\"},\n",
       " {'start': 36.0,\n",
       "  'end': 38.0,\n",
       "  'text': 'all of these models are based on transformers.'},\n",
       " {'start': 38.0,\n",
       "  'end': 41.0,\n",
       "  'text': 'So if you want to stay hip in machine learning,'},\n",
       " {'start': 41.0,\n",
       "  'end': 43.0,\n",
       "  'text': 'and especially in natural language processing,'},\n",
       " {'start': 43.0,\n",
       "  'end': 45.0,\n",
       "  'text': 'you have to know about the transformer.'},\n",
       " {'start': 45.0,\n",
       "  'end': 48.0,\n",
       "  'text': \"So in this video, I'm going to tell you about what transformers are,\"},\n",
       " {'start': 48.0,\n",
       "  'end': 50.0,\n",
       "  'text': \"how they work, and why they've been so impactful.\"},\n",
       " {'start': 50.0, 'end': 52.0, 'text': \"Let's get to it.\"},\n",
       " {'start': 52.0, 'end': 53.0, 'text': 'So what is a transformer?'},\n",
       " {'start': 53.0,\n",
       "  'end': 55.0,\n",
       "  'text': \"It's a type of neural network architecture.\"},\n",
       " {'start': 55.0,\n",
       "  'end': 58.0,\n",
       "  'text': 'To recap, neural networks are a very effective type of model'},\n",
       " {'start': 58.0,\n",
       "  'end': 63.0,\n",
       "  'text': 'of model for analyzing complicated data types like images, videos, audio, and text.'},\n",
       " {'start': 63.0,\n",
       "  'end': 66.0,\n",
       "  'text': 'But there are different types of neural networks optimized for different types of data.'},\n",
       " {'start': 66.0,\n",
       "  'end': 70.0,\n",
       "  'text': \"Like if you're analyzing images, you typically use a convolution neural network,\"},\n",
       " {'start': 70.0,\n",
       "  'end': 75.0,\n",
       "  'text': 'which is designed to vaguely mimic the way that the human brain processes vision.'},\n",
       " {'start': 75.0,\n",
       "  'end': 79.0,\n",
       "  'text': 'And since around 2012, neural networks have been really good at solving vision tasks,'},\n",
       " {'start': 79.0, 'end': 81.0, 'text': 'like identifying objects and photos.'},\n",
       " {'start': 81.0,\n",
       "  'end': 85.0,\n",
       "  'text': \"But for a long time, we didn't have anything comparably good for analyzing language,\"},\n",
       " {'start': 85.0,\n",
       "  'end': 87.0,\n",
       "  'text': 'whether for translation or text summarization or text generalization.'},\n",
       " {'start': 87.0, 'end': 89.0, 'text': 'or text generation.'},\n",
       " {'start': 89.0,\n",
       "  'end': 92.0,\n",
       "  'text': 'And this is a problem because language is the primary way that humans communicate.'},\n",
       " {'start': 92.0,\n",
       "  'end': 96.0,\n",
       "  'text': 'You see, until Transformers came around, the way we use deep learning to understand text'},\n",
       " {'start': 96.0,\n",
       "  'end': 102.0,\n",
       "  'text': 'was with a type of model called a recurrent neural network, or an RNN, that looks something like this.'},\n",
       " {'start': 102.0,\n",
       "  'end': 106.0,\n",
       "  'text': \"Let's say you wanted to translate a sentence from English to French.\"},\n",
       " {'start': 106.0,\n",
       "  'end': 110.0,\n",
       "  'text': 'An RNN would take as input an English sentence and process the words one at a time,'},\n",
       " {'start': 110.0,\n",
       "  'end': 113.0,\n",
       "  'text': 'and then sequentially spit out their French counterparts.'},\n",
       " {'start': 113.0, 'end': 116.0, 'text': 'The key word here is sequential.'},\n",
       " {'start': 116.0, 'end': 117.0, 'text': 'In language, the order of language,'},\n",
       " {'start': 117.0,\n",
       "  'end': 120.0,\n",
       "  'text': \"words matters and you can't just shuffle them around.\"},\n",
       " {'start': 120.0,\n",
       "  'end': 123.0,\n",
       "  'text': 'For example, the sentence, Jane went looking for trouble.'},\n",
       " {'start': 123.0,\n",
       "  'end': 125.0,\n",
       "  'text': 'It means something very different than the sentence,'},\n",
       " {'start': 125.0, 'end': 127.0, 'text': 'trouble went looking for Jane.'},\n",
       " {'start': 127.0,\n",
       "  'end': 130.0,\n",
       "  'text': \"So any model that's going to deal with language has to capture word order,\"},\n",
       " {'start': 130.0,\n",
       "  'end': 134.0,\n",
       "  'text': 'and recurrent neural networks do this by looking at one word at a time sequentially.'},\n",
       " {'start': 134.0, 'end': 136.0, 'text': 'But RNNs had a lot of problems.'},\n",
       " {'start': 136.0,\n",
       "  'end': 140.0,\n",
       "  'text': 'First, they never really did well at handling large sequences of text,'},\n",
       " {'start': 140.0, 'end': 142.0, 'text': 'like long paragraphs or essays.'},\n",
       " {'start': 142.0,\n",
       "  'end': 146.0,\n",
       "  'text': \"By the time they were analyzing the end of a paragraph, they'd forget what happened in the beginning.\"},\n",
       " {'start': 146.0, 'end': 147.0, 'text': 'And even worse,'},\n",
       " {'start': 147.0, 'end': 149.0, 'text': 'R&Ns were pretty hard to train.'},\n",
       " {'start': 149.0,\n",
       "  'end': 152.0,\n",
       "  'text': \"Because they processed words sequentially, they couldn't paralyze well,\"},\n",
       " {'start': 152.0,\n",
       "  'end': 156.0,\n",
       "  'text': \"which means that you couldn't just speed them up by throwing lots of GPUs at them.\"},\n",
       " {'start': 156.0,\n",
       "  'end': 160.0,\n",
       "  'text': \"And when you have a model that's slow to train, you can't train it on all that much data.\"},\n",
       " {'start': 160.0,\n",
       "  'end': 162.0,\n",
       "  'text': 'This is where the transformer changed everything.'},\n",
       " {'start': 162.0,\n",
       "  'end': 166.0,\n",
       "  'text': 'They were a model developed in 2017 by researchers at Google and the University of Toronto,'},\n",
       " {'start': 166.0,\n",
       "  'end': 169.0,\n",
       "  'text': 'and they were initially designed to do translation.'},\n",
       " {'start': 169.0,\n",
       "  'end': 173.0,\n",
       "  'text': 'But unlike recurrent neural networks, you could really efficiently paralyze transformers.'},\n",
       " {'start': 173.0,\n",
       "  'end': 176.0,\n",
       "  'text': 'And that meant that with the right hardware, you could train some really big models.'},\n",
       " {'start': 176.0, 'end': 177.0, 'text': 'models.'},\n",
       " {'start': 177.0, 'end': 178.0, 'text': 'How big?'},\n",
       " {'start': 178.0, 'end': 179.0, 'text': 'Really big.'},\n",
       " {'start': 179.0,\n",
       "  'end': 183.0,\n",
       "  'text': 'Remember GPT3, that model that writes poetry and code and has conversations?'},\n",
       " {'start': 183.0,\n",
       "  'end': 189.0,\n",
       "  'text': 'That was trained on almost 45 terabytes of text data, including like, almost the entire public'},\n",
       " {'start': 189.0, 'end': 190.0, 'text': 'web.'},\n",
       " {'start': 190.0,\n",
       "  'end': 193.0,\n",
       "  'text': 'So if you remember anything about Transformers, let it be this.'},\n",
       " {'start': 193.0,\n",
       "  'end': 198.0,\n",
       "  'text': 'Combine a model that scales really well with a huge data set, and the results will probably blow your'},\n",
       " {'start': 198.0, 'end': 199.0, 'text': 'mind.'},\n",
       " {'start': 199.0,\n",
       "  'end': 201.0,\n",
       "  'text': 'So how do these things actually work?'},\n",
       " {'start': 201.0,\n",
       "  'end': 204.0,\n",
       "  'text': 'From the diagram in the paper, it should be pretty clear.'},\n",
       " {'start': 204.0, 'end': 205.0, 'text': 'Or maybe not.'},\n",
       " {'start': 205.0, 'end': 206.0, 'text': 'Actually.'},\n",
       " {'start': 206.0, 'end': 208.0, 'text': \"it's simply than you might think.\"},\n",
       " {'start': 208.0,\n",
       "  'end': 211.0,\n",
       "  'text': 'There are three main innovations that make this model work so well.'},\n",
       " {'start': 211.0,\n",
       "  'end': 216.0,\n",
       "  'text': 'Positional encodings and attention, and specifically a type of attention called self-attention.'},\n",
       " {'start': 216.0,\n",
       "  'end': 219.0,\n",
       "  'text': \"Let's start by talking about the first one, positional encodings.\"},\n",
       " {'start': 219.0,\n",
       "  'end': 222.0,\n",
       "  'text': \"Let's say we're trying to translate text from English to French.\"},\n",
       " {'start': 222.0,\n",
       "  'end': 226.0,\n",
       "  'text': 'Positional encoding is the idea that instead of looking at words sequentially,'},\n",
       " {'start': 226.0,\n",
       "  'end': 229.0,\n",
       "  'text': 'you take each word in your sentence and before you feed it into the neural network,'},\n",
       " {'start': 229.0, 'end': 231.0, 'text': 'you slap a number on it.'},\n",
       " {'start': 231.0,\n",
       "  'end': 234.0,\n",
       "  'text': 'One, two, three, depending on what number the word is in the sentence.'},\n",
       " {'start': 234.0,\n",
       "  'end': 236.0,\n",
       "  'text': 'In other words, you store information about a word.'},\n",
       " {'start': 236.0,\n",
       "  'end': 240.0,\n",
       "  'text': 'word order in the data itself rather than in the structure of the network.'},\n",
       " {'start': 240.0,\n",
       "  'end': 244.4,\n",
       "  'text': 'Then as you train the network on lots of text data, it learns how to interpret those positional'},\n",
       " {'start': 244.4,\n",
       "  'end': 250.88,\n",
       "  'text': 'encodings. In this way, the neural network learns the importance of word order from the data.'},\n",
       " {'start': 250.88,\n",
       "  'end': 254.64,\n",
       "  'text': \"This is a high-level way to understand positional encodings, but it's an innovation that really\"},\n",
       " {'start': 254.64,\n",
       "  'end': 260.48,\n",
       "  'text': 'helped make transformers easier to train than RNMs. The next innovation in this paper is a concept'},\n",
       " {'start': 260.48,\n",
       "  'end': 265.28,\n",
       "  'text': \"called attention, which you'll see used everywhere in machine learning these days. In fact, the title of the\"},\n",
       " {'start': 265.28000000000003,\n",
       "  'end': 270.32000000000005,\n",
       "  'text': 'the original transformer paper is attention is all you need. So, the agreement on'},\n",
       " {'start': 270.32000000000005,\n",
       "  'end': 275.84000000000003,\n",
       "  'text': \"the European Economic Area was signed in August 1992. Did you know that? That's the\"},\n",
       " {'start': 275.84000000000003,\n",
       "  'end': 280.24,\n",
       "  'text': 'example sentence given in the original paper, and remember, the original transformer was designed'},\n",
       " {'start': 280.24,\n",
       "  'end': 285.28000000000003,\n",
       "  'text': 'for translation. Now imagine trying to translate that sentence to French. One bad way to'},\n",
       " {'start': 285.28000000000003,\n",
       "  'end': 290.24,\n",
       "  'text': 'translate text is to try to translate each word one for one. But in French, some words are flipped,'},\n",
       " {'start': 290.24,\n",
       "  'end': 295.20000000000005,\n",
       "  'text': 'like in the French translation, European comes before economic. Plus, French is a language.'},\n",
       " {'start': 295.28000000000003,\n",
       "  'end': 300.32000000000005,\n",
       "  'text': 'that has gendered agreement between words. So the word European needs to be in the feminine'},\n",
       " {'start': 300.32000000000005,\n",
       "  'end': 305.52000000000004,\n",
       "  'text': 'form to match with la zone. The attention mechanism is a neural network structure that allows a'},\n",
       " {'start': 305.52000000000004,\n",
       "  'end': 309.92,\n",
       "  'text': 'text model to look at every single word in the original sentence when making a decision about how'},\n",
       " {'start': 309.92,\n",
       "  'end': 314.72,\n",
       "  'text': \"to translate a word in the output sentence. In fact, here's a nice visualization from that paper\"},\n",
       " {'start': 314.72,\n",
       "  'end': 319.52000000000004,\n",
       "  'text': 'that shows what words in the input sentence the model is attending to when it makes predictions'},\n",
       " {'start': 319.52000000000004,\n",
       "  'end': 325.26000000000005,\n",
       "  'text': \"about a word for the output sentence. So when the model outputs the word European, it's\"},\n",
       " {'start': 325.28000000000003,\n",
       "  'end': 330.48,\n",
       "  'text': 'looking at the input words European and economic. You can think of this diagram as a sort of'},\n",
       " {'start': 330.48,\n",
       "  'end': 335.92,\n",
       "  'text': \"heat map for attention. And how does the model know which words it should be attending to? It's\"},\n",
       " {'start': 335.92,\n",
       "  'end': 340.56000000000006,\n",
       "  'text': \"something that's learned over time from data. By seeing thousands of examples of French and English\"},\n",
       " {'start': 340.56000000000006,\n",
       "  'end': 344.88000000000005,\n",
       "  'text': 'sentence pairs, the model learns about gender and word order and plurality and all of that'},\n",
       " {'start': 344.88000000000005,\n",
       "  'end': 349.92,\n",
       "  'text': 'grammatical stuff. So we talked about two key transformer innovations, positional encoding and'},\n",
       " {'start': 349.92,\n",
       "  'end': 355.12,\n",
       "  'text': 'attention. But actually, attention had been invented before this paper. The real innovation'},\n",
       " {'start': 355.12,\n",
       "  'end': 359.52,\n",
       "  'text': 'in Transformers with something called self-attention, a twist on traditional attention.'},\n",
       " {'start': 360.32,\n",
       "  'end': 364.32,\n",
       "  'text': 'The type of attention we just talked about had to do with aligning words in English and French,'},\n",
       " {'start': 364.32,\n",
       "  'end': 368.4,\n",
       "  'text': \"which is really important for translation. But what if you're just trying to understand the\"},\n",
       " {'start': 368.4,\n",
       "  'end': 373.28000000000003,\n",
       "  'text': 'underlying meaning in language so that you can build a network that can do any number of language'},\n",
       " {'start': 373.28000000000003,\n",
       "  'end': 378.48,\n",
       "  'text': \"tasks? What's incredible about neural networks like Transformers is that as they analyze\"},\n",
       " {'start': 378.48,\n",
       "  'end': 384.08,\n",
       "  'text': 'tons of text data, they begin to build up this internal representation or understanding of language'},\n",
       " {'start': 384.08,\n",
       "  'end': 389.52,\n",
       "  'text': 'automatically. They might learn, for example, that the words programmer and software engineer'},\n",
       " {'start': 389.52,\n",
       "  'end': 394.32,\n",
       "  'text': 'and software developer are all synonymous, and they might also naturally learn the rules of grammar'},\n",
       " {'start': 394.32,\n",
       "  'end': 399.03999999999996,\n",
       "  'text': 'and gender and tense and so on. The better this internal representation of language the neural'},\n",
       " {'start': 399.03999999999996,\n",
       "  'end': 404.15999999999997,\n",
       "  'text': 'network learns, the better it will be at any language task. And it turns out that attention can'},\n",
       " {'start': 404.15999999999997,\n",
       "  'end': 408.79999999999995,\n",
       "  'text': \"be a very effective way to get a neural network to understand language if it's turned on the input\"},\n",
       " {'start': 408.79999999999995,\n",
       "  'end': 413.68,\n",
       "  'text': 'text itself. Let me give you an example. Take these two sentences.'},\n",
       " {'start': 414.08,\n",
       "  'end': 418.32,\n",
       "  'text': 'Can I have the check versus looks like I just crashed the server?'},\n",
       " {'start': 418.32,\n",
       "  'end': 422.56,\n",
       "  'text': \"The word server here means two very different things, and I know that because I'm looking at the\"},\n",
       " {'start': 422.56,\n",
       "  'end': 427.91999999999996,\n",
       "  'text': 'context of the surrounding words. Self-attention allows a neural network to understand a word'},\n",
       " {'start': 427.91999999999996,\n",
       "  'end': 432.4,\n",
       "  'text': 'in the context of the words around it. So when a model processes the word server in the first'},\n",
       " {'start': 432.4,\n",
       "  'end': 438.0,\n",
       "  'text': 'sentence, it might be attending to the word check, which helps it disambiguate from a human server'},\n",
       " {'start': 438.0,\n",
       "  'end': 442.4,\n",
       "  'text': 'versus a metal one. In the second sentence, the model might be attending to the word crashed'},\n",
       " {'start': 442.40000000000003,\n",
       "  'end': 446.32000000000005,\n",
       "  'text': 'to determine that this server is a machine. Self-attention can also help neural networks'},\n",
       " {'start': 446.32000000000005,\n",
       "  'end': 452.16,\n",
       "  'text': 'disambiguate words, recognize parts of speech, and even identify word tense. This, in a nutshell,'},\n",
       " {'start': 452.16,\n",
       "  'end': 457.68000000000006,\n",
       "  'text': 'is the value of self-attention. So to summarize, transformers boil down to positional'},\n",
       " {'start': 457.68000000000006,\n",
       "  'end': 464.00000000000006,\n",
       "  'text': 'encodings, attention, and self-attention. Of course, this is a 10,000-foot look at transformers,'},\n",
       " {'start': 464.00000000000006,\n",
       "  'end': 468.56000000000006,\n",
       "  'text': 'but how are they actually useful? One of the most popular transformer-based models is called'},\n",
       " {'start': 468.56000000000006,\n",
       "  'end': 472.24,\n",
       "  'text': 'Bert, which was invented just around the time that I joined Google in 2018.'},\n",
       " {'start': 472.40000000000003,\n",
       "  'end': 477.28000000000003,\n",
       "  'text': 'Burt was trained on a massive text corpus and has become this sort of general pocket'},\n",
       " {'start': 477.28000000000003,\n",
       "  'end': 482.64000000000004,\n",
       "  'text': 'knife for NLP that can be adopted to a bunch of different tasks, like text summarization,'},\n",
       " {'start': 482.64000000000004,\n",
       "  'end': 486.94000000000005,\n",
       "  'text': 'question answering, classification, and finding similar sentences.'},\n",
       " {'start': 486.94000000000005,\n",
       "  'end': 491.34000000000003,\n",
       "  'text': \"It's used in Google Search to help understand search queries, and it powers a lot of Google Cloud's\"},\n",
       " {'start': 491.34000000000003,\n",
       "  'end': 495.12,\n",
       "  'text': 'NLP tools like Google Cloud, AutoMell natural language.'},\n",
       " {'start': 495.12,\n",
       "  'end': 499.8,\n",
       "  'text': 'Burt also proved that you could build very good models on unlabel data, like text scraped from'},\n",
       " {'start': 499.8, 'end': 501.6, 'text': 'Wikipedia or Reddit.'},\n",
       " {'start': 501.6,\n",
       "  'end': 507.36,\n",
       "  'text': \"This is called semi-supervised learning, and it's a big trend in machine learning right now.\"},\n",
       " {'start': 507.36,\n",
       "  'end': 511.96000000000004,\n",
       "  'text': \"So if I've sold you on how cool transformers are, you might want to start using them in your app.\"},\n",
       " {'start': 511.96000000000004,\n",
       "  'end': 513.0400000000001,\n",
       "  'text': 'No problem.'},\n",
       " {'start': 513.0400000000001,\n",
       "  'end': 517.0400000000001,\n",
       "  'text': 'Transferflow Hub is a great place to grab pre-trained transformer models like Bert.'},\n",
       " {'start': 517.0400000000001,\n",
       "  'end': 521.7,\n",
       "  'text': 'You can download them for free in multiple language and drop them straight into your app.'},\n",
       " {'start': 521.7,\n",
       "  'end': 525.64,\n",
       "  'text': 'You can also check out the popular Transformers Python Library, built by the company'},\n",
       " {'start': 525.64, 'end': 526.64, 'text': 'Hugging Face.'},\n",
       " {'start': 526.64,\n",
       "  'end': 530.0400000000001,\n",
       "  'text': \"That's one of the community's favorite ways to train and use transformer models.\"},\n",
       " {'start': 530.0400000000001,\n",
       "  'end': 531.58,\n",
       "  'text': 'For more transformer tips, check out my'},\n",
       " {'start': 531.6,\n",
       "  'end': 534.48,\n",
       "  'text': 'my blog post linked below and thanks for watching.'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8da04186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(transcript_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db7fa8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../notebook/output_frames/text_timestamp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad2a896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"../notebook/output_frames/frame_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "837d99bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The neat thing about working in machine learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>somebody invent something crazy that makes you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>like models that can play Go or generate hyper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>And today, the mind-blowing discovery that's r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>is a type of neural network called a transformer.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start   end                                               text\n",
       "0    0.0   3.0  The neat thing about working in machine learni...\n",
       "1    3.0   7.0  somebody invent something crazy that makes you...\n",
       "2    7.0  12.0  like models that can play Go or generate hyper...\n",
       "3   12.0  15.0  And today, the mind-blowing discovery that's r...\n",
       "4   15.0  17.0  is a type of neural network called a transformer."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00ae6e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scene</th>\n",
       "      <th>frame_count</th>\n",
       "      <th>frame_id</th>\n",
       "      <th>timestamp_sec</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>scene_000_frame_0000.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>1.960</td>\n",
       "      <td>scene_000_frame_0001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>3.921</td>\n",
       "      <td>scene_000_frame_0002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>141</td>\n",
       "      <td>5.881</td>\n",
       "      <td>scene_000_frame_0003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>188</td>\n",
       "      <td>7.841</td>\n",
       "      <td>scene_000_frame_0004.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scene  frame_count  frame_id  timestamp_sec                 file_name\n",
       "0      0            0         0          0.000  scene_000_frame_0000.jpg\n",
       "1      0            1        47          1.960  scene_000_frame_0001.jpg\n",
       "2      0            2        94          3.921  scene_000_frame_0002.jpg\n",
       "3      0            3       141          5.881  scene_000_frame_0003.jpg\n",
       "4      0            4       188          7.841  scene_000_frame_0004.jpg"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42383b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df_images = pd.read_csv(\"../notebook/output_frames/frame_metadata.csv\")  # has timestamp_sec\n",
    "df_texts = pd.read_csv(\"../notebook/output_frames/text_timestamp.csv\")    # has start, end, text\n",
    "\n",
    "# Create an empty list to store matches\n",
    "matched_rows = []\n",
    "\n",
    "# Iterate over each image frame\n",
    "for idx, img_row in df_images.iterrows():\n",
    "    frame_time = img_row['timestamp_sec']\n",
    "    \n",
    "    # Find all text segments where start ≤ timestamp ≤ end\n",
    "    matching_texts = df_texts[(df_texts['start'] <= frame_time) & (df_texts['end'] >= frame_time)]\n",
    "    \n",
    "    for tidx, txt_row in matching_texts.iterrows():\n",
    "        matched_rows.append({\n",
    "            'frame_id': img_row['frame_id'],\n",
    "            'file_name': img_row['file_name'],\n",
    "            'timestamp_sec': img_row['timestamp_sec'],\n",
    "            'text_id': tidx,\n",
    "            'start': txt_row['start'],\n",
    "            'end': txt_row['end'],\n",
    "            'text': txt_row['text']\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df_matched = pd.DataFrame(matched_rows)\n",
    "df_matched.to_csv(\"frame_text_crossmatch.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8eb076b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>timestamp_sec</th>\n",
       "      <th>text_id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>scene_000_frame_0000.jpg</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>The neat thing about working in machine learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>scene_000_frame_0001.jpg</td>\n",
       "      <td>1.960</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>The neat thing about working in machine learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94</td>\n",
       "      <td>scene_000_frame_0002.jpg</td>\n",
       "      <td>3.921</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>somebody invent something crazy that makes you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141</td>\n",
       "      <td>scene_000_frame_0003.jpg</td>\n",
       "      <td>5.881</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>somebody invent something crazy that makes you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188</td>\n",
       "      <td>scene_000_frame_0004.jpg</td>\n",
       "      <td>7.841</td>\n",
       "      <td>2</td>\n",
       "      <td>7.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>like models that can play Go or generate hyper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>12623</td>\n",
       "      <td>scene_045_frame_0007.jpg</td>\n",
       "      <td>526.484</td>\n",
       "      <td>142</td>\n",
       "      <td>525.64</td>\n",
       "      <td>526.64</td>\n",
       "      <td>Hugging Face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>12670</td>\n",
       "      <td>scene_045_frame_0008.jpg</td>\n",
       "      <td>528.445</td>\n",
       "      <td>143</td>\n",
       "      <td>526.64</td>\n",
       "      <td>530.04</td>\n",
       "      <td>That's one of the community's favorite ways to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>12704</td>\n",
       "      <td>scene_046_frame_0000.jpg</td>\n",
       "      <td>529.863</td>\n",
       "      <td>143</td>\n",
       "      <td>526.64</td>\n",
       "      <td>530.04</td>\n",
       "      <td>That's one of the community's favorite ways to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>12751</td>\n",
       "      <td>scene_046_frame_0001.jpg</td>\n",
       "      <td>531.823</td>\n",
       "      <td>145</td>\n",
       "      <td>531.60</td>\n",
       "      <td>534.48</td>\n",
       "      <td>my blog post linked below and thanks for watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>12798</td>\n",
       "      <td>scene_046_frame_0002.jpg</td>\n",
       "      <td>533.783</td>\n",
       "      <td>145</td>\n",
       "      <td>531.60</td>\n",
       "      <td>534.48</td>\n",
       "      <td>my blog post linked below and thanks for watch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>289 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     frame_id                 file_name  timestamp_sec  text_id   start  \\\n",
       "0           0  scene_000_frame_0000.jpg          0.000        0    0.00   \n",
       "1          47  scene_000_frame_0001.jpg          1.960        0    0.00   \n",
       "2          94  scene_000_frame_0002.jpg          3.921        1    3.00   \n",
       "3         141  scene_000_frame_0003.jpg          5.881        1    3.00   \n",
       "4         188  scene_000_frame_0004.jpg          7.841        2    7.00   \n",
       "..        ...                       ...            ...      ...     ...   \n",
       "284     12623  scene_045_frame_0007.jpg        526.484      142  525.64   \n",
       "285     12670  scene_045_frame_0008.jpg        528.445      143  526.64   \n",
       "286     12704  scene_046_frame_0000.jpg        529.863      143  526.64   \n",
       "287     12751  scene_046_frame_0001.jpg        531.823      145  531.60   \n",
       "288     12798  scene_046_frame_0002.jpg        533.783      145  531.60   \n",
       "\n",
       "        end                                               text  \n",
       "0      3.00  The neat thing about working in machine learni...  \n",
       "1      3.00  The neat thing about working in machine learni...  \n",
       "2      7.00  somebody invent something crazy that makes you...  \n",
       "3      7.00  somebody invent something crazy that makes you...  \n",
       "4     12.00  like models that can play Go or generate hyper...  \n",
       "..      ...                                                ...  \n",
       "284  526.64                                      Hugging Face.  \n",
       "285  530.04  That's one of the community's favorite ways to...  \n",
       "286  530.04  That's one of the community's favorite ways to...  \n",
       "287  534.48  my blog post linked below and thanks for watch...  \n",
       "288  534.48  my blog post linked below and thanks for watch...  \n",
       "\n",
       "[289 rows x 7 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2547bd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:25<00:00, 11.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"frame_text_crossmatch.csv\")\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Cache embeddings to avoid recomputation\n",
    "image_cache = {}\n",
    "text_cache = {}\n",
    "\n",
    "# Store combined embeddings\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    img_path = row['file_name']\n",
    "    text = row['text']\n",
    "    \n",
    "    # Get image embedding\n",
    "    if img_path not in image_cache:\n",
    "        img_path = \"../notebook/output_frames/\" + img_path\n",
    "        image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = model.encode_image(image).squeeze()\n",
    "            image_embedding = image_embedding / image_embedding.norm()\n",
    "        image_cache[img_path] = image_embedding\n",
    "    else:\n",
    "        image_embedding = image_cache[img_path]\n",
    "    \n",
    "    # Get text embedding\n",
    "    if text not in text_cache:\n",
    "        with torch.no_grad():\n",
    "            text_token = clip.tokenize([text]).to(device)\n",
    "            text_embedding = model.encode_text(text_token).squeeze()\n",
    "            text_embedding = text_embedding / text_embedding.norm()\n",
    "        text_cache[text] = text_embedding\n",
    "    else:\n",
    "        text_embedding = text_cache[text]\n",
    "\n",
    "    # Max pooling fusion\n",
    "    image_embeddings.append(image_embedding.cpu().numpy())\n",
    "    text_embeddings.append(text_embedding.cpu().numpy())\n",
    "\n",
    "# Convert list to numpy matrix for cosine similarity\n",
    "# Store for later indexing\n",
    "# df[\"embedding_index\"] = range(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45db8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, dim=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, text_emb, image_emb):\n",
    "        # Inputs: [B, dim] → reshape to [B, 1, dim]\n",
    "        text = text_emb.unsqueeze(1)\n",
    "        image = image_emb.unsqueeze(1)\n",
    "\n",
    "        # text attends to image\n",
    "        fused, _ = self.cross_attn(query=text, key=image, value=image)\n",
    "        fused = self.norm(fused + text)  # residual + norm\n",
    "        return self.mlp(fused.squeeze(1))  # [B, dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acf91083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(image_feats, text_feats, temperature=0.07):\n",
    "    image_feats = F.normalize(image_feats, dim=-1)\n",
    "    text_feats = F.normalize(text_feats, dim=-1)\n",
    "    logits = image_feats @ text_feats.T / temperature\n",
    "    labels = torch.arange(len(image_feats)).to(image_feats.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i2t + loss_t2i) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ba2969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cross_attention(image_embeddings, text_embeddings, epochs=100, lr=1e-4, batch_size=64):\n",
    "    model = CrossAttentionFusion(dim=512).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(image_embeddings).float(),\n",
    "        torch.tensor(text_embeddings).float()\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for img_emb, txt_emb in loader:\n",
    "            img_emb, txt_emb = img_emb.to(model.mlp[0].weight.device), txt_emb.to(model.mlp[0].weight.device)\n",
    "            fused_i = model(txt_emb, img_emb)\n",
    "            fused_t = model(txt_emb, img_emb)  # same for both\n",
    "\n",
    "            loss = contrastive_loss(fused_i, fused_t)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71a55a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_fused_embeddings(model, image_embeddings, text_embeddings):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img_tensor = torch.tensor(image_embeddings).float().to(model.mlp[0].weight.device)\n",
    "        txt_tensor = torch.tensor(text_embeddings).float().to(model.mlp[0].weight.device)\n",
    "        fused = model(txt_tensor, img_tensor)\n",
    "    return fused.cpu().numpy()\n",
    "\n",
    "def search(query_embedding, fused_embeddings, meta_data, top_k=5):\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "    sims = cosine_similarity([query_embedding], fused_embeddings)[0]\n",
    "    top_idx = sims.argsort()[::-1][:top_k]\n",
    "    return [meta_data[i] for i in top_idx], sims[top_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "755a215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"frame_text_crossmatch.csv\")  # ← put correct path here\n",
    "\n",
    "# Combine image paths and texts per row\n",
    "image_paths = df['file_name'].tolist()\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "# Optional metadata (frame_id, time etc.)\n",
    "metadata = df[['file_name', 'text', 'timestamp_sec']].to_dict(orient='records')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1606660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(clip_model, query_text, device):\n",
    "    with torch.no_grad():\n",
    "        tokens = clip.tokenize([query_text]).to(device)\n",
    "        text_emb = clip_model.encode_text(tokens).squeeze()\n",
    "    return text_emb / text_emb.norm()\n",
    "\n",
    "def get_image_embedding(clip_model, image_path, preprocess, device):\n",
    "    from PIL import Image\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_emb = clip_model.encode_image(image).squeeze()\n",
    "    return img_emb / img_emb.norm()\n",
    "\n",
    "def search_by_text(query_text, model, clip_model, fused_db, metadata, device):\n",
    "    tokens = clip.tokenize([query_text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_emb = clip_model.encode_text(tokens).squeeze(0)\n",
    "        dummy_img = torch.zeros_like(text_emb)\n",
    "        fused_query = model(text_emb.unsqueeze(0), dummy_img.unsqueeze(0)).squeeze(0)\n",
    "    return search(fused_query.detach().cpu().numpy(), fused_db, metadata)\n",
    "\n",
    "\n",
    "def search_by_image(image_path, model, clip_model, preprocess, fused_db, meta_data, device):\n",
    "    img_emb = get_image_embedding(clip_model, image_path, preprocess, device)\n",
    "    dummy_txt = torch.zeros_like(img_emb)\n",
    "    fused_query = model(dummy_txt.unsqueeze(0), img_emb.unsqueeze(0)).squeeze().cpu().numpy()\n",
    "    return search(fused_query, fused_db, meta_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5d884f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 5.0783\n",
      "Epoch 2/10, Loss: 0.8889\n",
      "Epoch 3/10, Loss: 0.6298\n",
      "Epoch 4/10, Loss: 0.5946\n",
      "Epoch 5/10, Loss: 0.4154\n",
      "Epoch 6/10, Loss: 0.4787\n",
      "Epoch 7/10, Loss: 0.5167\n",
      "Epoch 8/10, Loss: 0.4321\n",
      "Epoch 9/10, Loss: 0.3344\n",
      "Epoch 10/10, Loss: 0.3326\n"
     ]
    }
   ],
   "source": [
    "# 1. Train model\n",
    "model = train_cross_attention(image_embeddings, text_embeddings, epochs=10)\n",
    "\n",
    "# 2. Compute fused embedding database\n",
    "fused_db = compute_fused_embeddings(model, image_embeddings, text_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88ee398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': 'scene_005_frame_0001.jpg', 'text': 'like identifying objects and photos.', 'timestamp_sec': 80.622} → Score: 0.248\n",
      "{'file_name': 'scene_045_frame_0000.jpg', 'text': 'No problem.', 'timestamp_sec': 512.762} → Score: 0.229\n",
      "{'file_name': 'scene_009_frame_0003.jpg', 'text': 'trouble went looking for Jane.', 'timestamp_sec': 125.417} → Score: 0.227\n",
      "{'file_name': 'scene_035_frame_0005.jpg', 'text': \"The word server here means two very different things, and I know that because I'm looking at the\", 'timestamp_sec': 421.463} → Score: 0.221\n",
      "{'file_name': 'scene_035_frame_0003.jpg', 'text': 'Can I have the check versus looks like I just crashed the server?', 'timestamp_sec': 417.542} → Score: 0.209\n"
     ]
    }
   ],
   "source": [
    "# 3. Search\n",
    "results, scores = search_by_text(\"cat \", model, clip_model, fused_db, metadata, device)\n",
    "for item, score in zip(results, scores):\n",
    "    print(item, \"→ Score:\", round(score, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
